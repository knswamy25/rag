{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG (Retrieval-Augmented Generation) with Langchain\n",
    "\n",
    "## Overview\n",
    "\n",
    "Implements a basic Retrieval-Augmented Generation (RAG) system for processing and querying PDF documents. The system encodes the document content into a vector store, which can then be queried to retrieve relevant information.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "The Basic Retrieval-Augmented Generation (RAG) Pipeline operates through two main phases:\n",
    "\n",
    "1. Data Indexing\n",
    "2. Retrieval & Generation\n",
    "\n",
    "\n",
    "### Data Indexing Process:\n",
    "a. Data Loading: Importing all the documents or information to be utilized.\n",
    "b. Document Splitting: Documents are divided into smaller chunks, for instance, sections of no more than 500 characters each.\n",
    "c. Data Embedding: The data is converted into vectors using an embedding model.\n",
    "d. Data Storing: Embeddings are saved in a vector database, allowing them to be easily searched.\n",
    "\n",
    "### Retrieval and Generation Process:\n",
    "\n",
    "Retrieval: When a user asks a question:\n",
    "The user’s input is first transformed into a vector (query vector) using the same embedding model from the Data Indexing phase.\n",
    "This query vector is then matched against all vectors in the vector database to find the most similar ones (e.g., using the Euclidean distance metric) that might contain the answer to the user’s question. This step is about identifying relevant knowledge chunks.\n",
    "2. Generation: The LLM model takes the user’s question and the relevant information retrieved from the vector database to create a response. This process combines the question with the identified data to generate an answer.\n",
    "\n",
    "## Implementation Details\n",
    "\n",
    "### Data Loading\n",
    "\n",
    "1. The PDF is loaded using PyPDFLoader.\n",
    "2. The text is split into chunks using RecursiveCharacterTextSplitter with specified chunk size and overlap.\n",
    "\n",
    "### Text Cleaning\n",
    "\n",
    "A custom function `replace_tab_with_space` is applied to clean the text chunks. This likely addresses specific formatting issues in the PDF.\n",
    "\n",
    "### Vector Store Creation\n",
    "\n",
    "1. Huggingface embeddings are used to create vector representations of the text chunks.\n",
    "2. A Chroma vector store is created from these embeddings for efficient similarity search.\n",
    "\n",
    "### Encoding Function\n",
    "\n",
    "The `encode_pdf` function encapsulates the entire process of loading, chunking, cleaning, and encoding the PDF into a vector store.\n",
    "\n",
    "## Usage Example\n",
    "\n",
    "The code includes a test query: \"What is the main cause of climate change?\". This demonstrates how to use the retriever to fetch relevant context from the processed document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the OpenAI API key environment variable (comment out if not using OpenAI)\n",
    "if not os.getenv('OPENAI_API_KEY'):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = input(\"Please enter your OpenAI API key: \")\n",
    "else:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..'))) # Add the parent directory to the path since we work with notebooks\n",
    "# This pdf is generated from https://www.adyen.com/knowledge-hub/llm-inference-at-scale-with-tgi\n",
    "path = \"../data/LLM_Inference_TGI_Adyen.pdf\"\n",
    "print(f\"Document path = {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Document Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "#from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "# ----- Data Indexing Process -----\n",
    "\n",
    "def replace_tab_with_space(list_of_documents):\n",
    "    \"\"\"\n",
    "    Replaces all tab characters ('\\t') with spaces in the page content of each document.\n",
    "\n",
    "    Args:\n",
    "        list_of_documents: A list of document objects, each with a 'page_content' attribute.\n",
    "\n",
    "    Returns:\n",
    "        The modified list of documents with tab characters replaced by spaces.\n",
    "    \"\"\"\n",
    "\n",
    "    for doc in list_of_documents:\n",
    "        doc.page_content = doc.page_content.replace('\\t', ' ')  # Replace tabs with spaces\n",
    "    return list_of_documents\n",
    "    \n",
    "\n",
    "def convert_pdf_to_embeddings(path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Converts a PDF book into a vector store using OpenAI embeddings.\n",
    "\n",
    "    Args:\n",
    "        path: The path to the PDF file.\n",
    "        chunk_size: The desired size of each text chunk.\n",
    "        chunk_overlap: The amount of overlap between consecutive chunks.\n",
    "\n",
    "    Returns:\n",
    "        A FAISS vector store containing the encoded book content.\n",
    "    \"\"\"\n",
    "\n",
    "    # load your pdf doc\n",
    "    loader = PyPDFLoader(path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
    "    )\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    #print(len(texts))\n",
    "    cleaned_texts = replace_tab_with_space(texts)\n",
    "\n",
    "    # Create embeddings (Tested with OpenAI and Amazon Bedrock)\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "    \n",
    "    # Create Chroma vector store\n",
    "    #vectorstore = Chroma.from_documents(documents, OpenAIEmbeddings())\n",
    "    vectorstore = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Vector Store\n",
    "chunks_vector_store = convert_pdf_to_embeddings(path, chunk_size=500, chunk_overlap=50)\n",
    "print(type(chunks_vector_store))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To better illustrate how TGI's continuous batching algorithm works, let's walk through a\n",
      "specific example with the following initial setup seen in Table 1. Initially, no requests are\n",
      "being processed so the total token budget is equal to MBT.\n",
      "prefill(batch)\n",
      "# Main loop to manage requests\n",
      "while batch:\n",
      "# Update the token budget based on current batch\n",
      "batch_max_tokens = sum(request.input_tokens + request.max_new_tokens for request in batch)\n",
      "token_budget = max_batch_total_tokens - batch_max_tokens\n"
     ]
    }
   ],
   "source": [
    "# Run query and get the answer\n",
    "query = \"What is TGI?\"\n",
    "docs = chunks_vector_store.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
